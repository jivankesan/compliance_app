from langchain_experimental.text_splitter import SemanticChunker
from langchain.text_splitter import RecursiveCharacterTextSplitter

class TextProcessor:
    """
    Handles splitting of text into chunks using a configurable approach.
    """
    def __init__(self, embedding_model, threshold_type="percentile", threshold_amount=88.0):
        self.text_splitter = SemanticChunker(
            embedding_model,
            breakpoint_threshold_type=threshold_type,
            breakpoint_threshold_amount=threshold_amount
        )

    def create_chunks(self, text: str):
        """
        Creates chunks from text using the SemanticChunker.
        Returns a list of Document objects (LangChain style).
        """
        docs = self.text_splitter.create_documents([text])
        return docs

    # Alternative (backup) approach if needed:
    # def create_recursive_chunks(self, text: str, chunk_size=1000, chunk_overlap=200):
    #     splitter = RecursiveCharacterTextSplitter(
    #         chunk_size=chunk_size,
    #         chunk_overlap=chunk_overlap
    #     )
    #     docs = splitter.create_documents([text])
    #     return docs